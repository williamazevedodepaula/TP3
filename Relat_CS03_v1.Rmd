---
title: "Estudo de Caso 3"
author: "Andre Avelar, Brayan Jaimes, Marcelo Carneiro, William de Paula"
date: "07 de novembro de 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Estudo de Caso 3

## Introdução

Algoritmos computacionais são amplamente utilizados em problemas de otimização em engenharia. A otimização busca obter um ponto ótimo para uma dada função-objetivo, podendo este ponto ser um mínimo ou um máximo conforme o problema a ser otimizado. Entre os algoritmos disponíveis, temos aqueles baseados em populações, que consistem de um ciclo iterativo no qual um conjunto de soluções-candidatas são repetidamente sujeitas a operadores de variação e seleção, promovendo uma exploração do espaço de variáveis em busca do ponto ótimo.   

Um pesquisador implementou um algoritmo baseado em populações utilizando o método *differential evolution* [1] e diversos operadores de forma padronizada no pacote ExpDE [2] do *RStudio*.   

O presente estudo de caso tem o objetivo de comparar experimentalmente quatro diferentes configurações deste algoritmo para uma única função-objetivo. A comparação será feita através do desempenho médio do algoritmo, sendo que quanto menor o valor retornado, melhor o algoritmo. Adicionalmente, devemos responder às seguintes perguntas:   

- Há alguma diferença no desempenho médio do algoritmo para as diferentes configurações?   
- Caso haja diferença, qual a melhor configuração em termos de desempenho médio, e qual a magnitude das diferenças encontradas?   
- Há alguma configuração que deva ser recomendada em relação às demais?   

Para este estudo de caso, os papéis desempenhados por cada membro da equipe *Los Paisas* foram:   

- *Coordenador*: Brayan    
- *Relator*: André    
- *Verificador*: William     
- *Monitor*: Marcelo     

O programa *RStudio* foi utilizado para a realização dos cálculos e dos gráficos deste estudo de caso, e o *R Markdown* foi utilizado para a consolidação e elaboração deste relatório.   

## Atividades

### 1. Formulação das Hipóteses de Teste   

O objetivo é determinar se há alguma diferença no desempenho médio do algoritmo para as diferentes configurações. Portanto, para esta análise foram definidas as seguintes hipóteses de teste:   

$$  
  \begin{cases}
    H_0: \tau_{i} = 0, \forall \  i\\
    H_1: \exists \ \tau_{i} \neq  0
  \end{cases}
$$

Se for possível rejeitar a hipótese nula ($H_0$), podemos afirmar com um certo nível de confiança que existe alguma diferença no desempenho médio do algoritmo para as diferentes configurações.   

Utilizaremos o teste da distribuição tipo **F**.   

$$ F_0=\frac{MS_{níveis}}{MS_E} \ \ \Rightarrow \ \ F_{(a-1),a(n-1)} \ \ graus \ de \ liberdade $$
Onde:   
$MS_{níveis}=\frac{SS_{níveis}}{a-1}$ representa os quadrados médios dos níveis, dado pela soma dos quadrados dos níveis dividido pelo respectivo graus de liberdade.   
$MS_E=\frac{SS_E}{a(n-1)}$ representa os quadrados médios dos resíduos, dado pela soma dos quadrados dos resíduos dividido pelo respectivo graus de liberdade.   

Rejeitaremos $H_0$ com o nível de significância $\alpha$ se $f_0$ > $F^{1-\alpha}_{(a-1),a(n-1)}$.   

Este teste pode ser executado no *RStudio* utilizando os comandos 'aov' e posteriormente 'summary.aov' (denominado tabela ANOVA), que retorna o valor da estatística de teste $F_0$ e o valor da probabilidade de ser maior que $F_0$ (ou p-valor). Neste caso, rejeitaremos $H_0$ com o nível de significância $\alpha$ se p-valor < $\alpha$.    


### 2. Cálculo do Tamanho Amostral    

Os seguintes parâmetros experimentais foram fornecidos:    

- Mínima diferença de importância prática padronizada em termos do coeficiente *d* de Cohen: $d^*=\delta^*/\sigma=0.25$, o que significa que a razão delta pelo desvio padrão deve ser igual a 0.25   
- Nível de significância: $\alpha=0.05$    
- Potência mínima: $\pi=0.85$    

```{r, echo=TRUE}

a<-4
k<-(a*(a-1))/2
alpha<-0.05
alpha_adj<-alpha/k
delta<-10
potencia_desejada<-0.85
sd<-40
```

Estamos testando quatro configurações diferentes do mesmo algoritmo, o que nos leva a um fator experimental único (tipo de configuração) com *a* = 4 níveis diferentes (Config 1, Config 2, Config 3 e Config 4) e número de réplicas *n* em cada nível sendo calculado na sequência.   

Um dos objetivos do estudo de caso é determinar qual configuração é melhor do que as outras, sem interesse especial em uma configuração específica. Desta forma, após realizar o teste ANOVA, devemos realizar comparações de *todos* vs. *todos*. Neste caso, o número de comparações K é calculado por:   

$$ K=\frac{a(a-1)}{2} $$

Assim, K = `r k`. Com o intuito de manter a taxa geral de erro controlada no valor desejado $\alpha$ = `r alpha`, devemos ajustar o valor $\alpha$ usado em cada teste de comparação de par de configurações e também no cálculo do tamanho amostral. Utilizando  o método de correção de Bonferroni, o valor $\alpha_{ajust}$ é calculado como:   

$$ \alpha_{ajust}=\frac{\alpha_{família}}{K} $$
Assim, $\alpha_{ajust}$ = `r alpha_adj`.    

Com estas informações, utilizamos o comando 'power.t.test' para duas amostras do *RStudio* para calcular o número de réplicas *n* em cada nível, ou seja, o tamanho amostral.    

```{r, echo=TRUE}

test_pow_N <- power.t.test(delta = delta, power = potencia_desejada, sd = sd, sig.level = alpha_adj, alternative = "two.sided", type = "two.sample")
print(test_pow_N)
```

Com este resultado, o tamanho amostral em cada nível é *n* = `r ceiling(test_pow_N$n)`.    


### 3. Coleta e Tabulação dos Dados   

Após o cálculo do número de réplicas em cada nível, ou seja, o tamanho amostral, fizemos a coleta dos dados utilizando os parâmetros fixos e os parâmatros de cada equipe através dos comandos descritos na sequência.   


if(!require(ExpDE)){   
  install.packages("ExpDE")   
  library(ExpDE)   
}   

selpars  <- list(name  = "selection_standard")   
stopcrit <- list(names = "stop_maxeval", maxevals = 50000, maxiter = 1000)   
probpars <- list(name  = "sphere", xmin  = -seq(1,20), xmax  = 20 + 5 * seq(5, 24))   


%# Equipe Marcelo, Brayan, William, André   

%## Config 1   
recpars1 <- list(name = "recombination_blxAlphaBeta", alpha = 0, beta = 0)   
mutpars1 <- list(name = "mutation_rand", f = 4)   
popsize1 <- 200   

%## Config 2   
recpars2 <- list(name = "recombination_exp", cr = 0.6)   
mutpars2 <- list(name = "mutation_best", f = 2)   
popsize2 <- 130   

%## Config 3   
recpars3 <- list(name = "recombination_blxAlphaBeta", alpha = 0.4, beta = 0.4)    
mutpars3 <- list(name = "mutation_rand", f = 4)   
popsize3 <- 230   

%## Config 4   
recpars4 <- list(name = "recombination_wright")   
mutpars4 <- list(name = "mutation_best", f = 4.8)   
popsize4 <- 113   

%# Run algorithm on problem:   
Numero_it<-434   
result_out1<-list()   
result_out2<-list()   

for (i in 1:Numero_it) {   
  print(i)   
  out1 <- ExpDE(popsize  = popsize1,    
                mutpars  = mutpars1,    
                recpars  = recpars1,    
                selpars  = selpars,    
                stopcrit = stopcrit,    
                probpars = probpars,   
                showpars = list(show.iters = "dots",   
                                showevery  = 20))   
  
  out2 <- ExpDE(popsize  = popsize2,    
                mutpars  = mutpars2,    
                recpars  = recpars2,    
                selpars  = selpars,    
                stopcrit = stopcrit,    
                probpars = probpars,   
                showpars = list(show.iters = "dots",   
                                showevery  = 20))   
  
  out3 <- ExpDE(popsize  = popsize3,    
                mutpars  = mutpars3,    
                recpars  = recpars3,    
                selpars  = selpars,    
                stopcrit = stopcrit,    
                probpars = probpars,   
                showpars = list(show.iters = "dots",   
                                showevery  = 20))   
  
  out4 <- ExpDE(popsize  = popsize4,    
                mutpars  = mutpars4,    
                recpars  = recpars4,    
                selpars  = selpars,    
                stopcrit = stopcrit,    
                probpars = probpars,   
                showpars = list(show.iters = "dots",   
                                showevery  = 20))   

  
  result_out1[[i]]<-c(out1$Fbest,out2$Fbest,out3$Fbest,out4$Fbest)   
  result_out2[[i]]<-c("out1","out2","out3","out4")   
}   

dados_total<-data.frame(label_out=(unlist(result_out2)),out=(unlist(result_out1)))   
write.csv(dados_total, file = "dados_total.csv")   
   

A coleta de dados inicial resultou no arquivo "dados_total.csv". Uma vez que a cada execução o algoritmo retorna um valor diferente, não é recomendado executar a rotina acima novamente. Se a rotina for executada novamente, teremos novos valores amostrados. Por esta razão, a rotina está em formato de texto e não em formato de código R executável neste relatório.   

A importação dos dados foi executada utilizando o comando 'read_delim', e a análise exploratória inicial dos dados com os comandos 'summary', 'head' e 'boxplot'.   

```{r, echo=FALSE, results='hide'}
if(!require(car)){
    install.packages("car")
    library(car)
}
getwd()
```

```{r, echo=TRUE,fig.align='center',out.width = '80%'}
algoritmo <- read.table(file = "dados_total.csv", header = TRUE, sep = ",")
summary(algoritmo)
head(algoritmo, 8)
boxplot(out~label_out, data = algoritmo, xlab = "Configuração", ylab = "Desempenho", main = "Dados de Desempenho do Algoritmo", pch = 16, col = "green")
```

O boxplot sugere que: (i) a configuração 2 possui um desempenho melhor do que as demais configurações; (ii) as configurações 1, 3 e 4 possuem  desempenho similar; (iii) todas as configurações possuem simetria dos dados e (iv) possíveis *outliers* devem ser levados em consideração, principalmente nas configurações 3 e 4.    


### 4. Teste das Hipóteses   

O teste das hipóteses foi realizado no *RStudio* utilizando os comandos 'aov' e 'summary.aov', conforme descrito na seção 1.   

```{r, echo=TRUE}
model <- aov(out~label_out, data = algoritmo)
summary.aov(model)
```

O resultado do teste das hipóteses retornou um p-valor < 2 x $10^{-16}$. Como o p-valor é menor do que o valor de $\alpha$ = `r alpha`, temos evidência suficiente para rejeitar $H_0$ com o nível de significância de `r alpha`. Desta forma, concluímos que existe alguma diferença no desempenho médio do algoritmo para as diferentes configurações.   


### 5. Verificação das Premissas dos Testes   

O teste de hipóteses realizado na seção 4 assume as premissas de normalidade, homoscedasticidade e independência dos resíduos. Vamos agora verificar se estas premissas foram atendidas.   

A premissa de normalidade pode ser verificada através do teste de Shapiro-Wilk em conjunto com o gráfico tipo qqPlot.

```{r, echo=TRUE,fig.align='center',out.width = '80%'}
shapiro.test(model$residuals)
sht <- shapiro.test(model$residuals)
qqPlot(model$residuals, pch = 16, lwd = 3, cex = 1, las = 1, xlab = "Quantis Normalizados", ylab = "Resíduos", main = "Gráfico de Normalidade dos Resíduos")
```
```{r, echo=FALSE,fig.align='center',out.width = '80%'}
cat("p-valor: ",sht$p.value)
```

O resultado do teste de Shapiro-Wilk retornou um p-valor = $`r sht$p.value`$. Este p-valor muito baixo nos leva a rejeitar o teste de normalidade e concluir que os resíduos não possuem uma distribuição normal. No gráfico qqPlot, observamos que os dados estão fora da linha de normalidade, com valores abaixo da linha para quantis baixos e acima da linha para quantis altos. Este tipo de gráfico representa distibuições não normais com caudas leves.     

A premissa de homoscedasticidade pode ser verificada através do teste de Fligner-Killeen em conjunto com o gráfico por valores ajustados.   

```{r, echo=TRUE,fig.align='center',out.width = '80%'}
fligner.test(out~label_out, data = algoritmo)
flt <- fligner.test(out~label_out, data = algoritmo)
plot(x = model$fitted.values, y = model$residuals, xlab = "Valores Ajustados", ylab = "Resíduos", main = "Gráfico de Homoscedasticidade dos Resíduos")
```
```{r, echo=FALSE,fig.align='center',out.width = '80%'}
cat("p-valor: ",flt$p.value)
```

O resultado do teste de Fligner-Killeen retornou um p-valor = $`r flt$p.value`$. Este p-valor muito baixo nos leva a rejeitar o teste de  homoscedasticidade e concluir que os resíduos possuem heteroscedasticidade. No gráfico por valores ajustados, observamos uma forma visual do tipo "megafone", onde para maiores valores ajustados no eixo x, temos maiores valores de resíduos no eixo y. Esta forma de gráfico caracteriza desigualdade de variâncias entre grupos, ou heteroscedasticidade.   

A premissa de independência foi observada na fase de planejamento, onde a avaliação do algoritmo foi executada através de amostragem sequencial da configuração 1 até a configuração 4 (amostra 1 da config 1, amostra 1 da config 2, amostra 1 da config 3, amostra 1 da config 4; amostra 2 da config 1, amostra 2 da config 2, amostra 2 da config 3, amostra 2 da config 4; e assim por diante), ao invés de amostrar todos os valores da configuração 1 até a configuração 4 (434 amostras da config 1, 434 amostras da config 2, 434 amostras da config 3 e 434 amostras da config 4). Esta premissa também pode ser verificada através do teste de Durbin-Watson para correlações seriais em conjunto com o gráfico de ordenação dos resíduos.

```{r, echo=TRUE,fig.align='center',out.width = '80%'}
durbinWatsonTest(model)
dwt <- durbinWatsonTest(model)
plot(x = seq_along(model$residuals), y = model$residuals, type = "l", xlab = "Ordem dos Resíduos", ylab = "Valor dos Resíduos", main = "Gráfico de Ordenação dos Resíduos")
points(x = seq_along(model$residuals), y = model$residuals, type = "p", col = as.numeric(algoritmo[,1]))
```
```{r, echo=FALSE,fig.align='center',out.width = '80%'}
cat("p-valor: ",dwt$p)
```

O teste de Durbin-Watson retornou um p-valor = $`r dwt$p`$. Este p-valor indica independência dos resíduos. O gráfico de ordenação dos resíduos não possui tendência de alta ou de baixa, indicando a independência dos resíduos.   

Os resultados desta seção demonstram que as premissas de normalidade e de homoscedasticidade dos resíduos não foram atendidas. Por isto, vamos realizar a adequação destas premissas na próxima seção.   


### 6. Adequação das Premissas dos Testes

Como as premissas de homoscedasticidade e de normalidade não foram atendidas e a análise de variância (ANOVA) depdende dessas premissas, mostrou-se necessário o uso de técnicas não paramétricas, pois essas assumem poucas (ou nenhuma) hipóteses sobre a distribuição de probabilidade da população [3].
Como alternativa ao ANOVA, o teste não-paramétrico de Kruskal-Wallis pode ser utilizado, o qual realiza a comparação entre médias de 3 ou mais amostras independentes [4]. O teste de kruskal-wallis é análogo ao ANOVA 1 fator, porém sem nenhuma restrição sobre as premissas de normalidade e homoscedasticidade.

No R studio, o teste de kruskal-wallis pode ser executado através da função kruskal.test:

```{r, echo=TRUE}
kw_model <- kruskal.test(out~label_out, data = algoritmo)
print(kw_model)
```

O resultado do teste das hipóteses retornou um p-valor = $`r kw_model$p.value`$. Como o p-valor é menor do que o valor de $\alpha$ = `r alpha`, temos evidência suficiente para rejeitar $H_0$ com o nível de significância de `r alpha`. Desta forma, concluímos que existe alguma diferença no desempenho médio do algoritmo para as diferentes configurações.

### 7. Estimação dos Tamanhos de Efeito e dos Intervalos de Confiança   

Como o teste utilizado foi o não-paramétrico de Kruskal-Wallis, o mesmo não utiliza estimativas de parâmetros amostrais e, por isso, não possui intervalo de confiança.

Para efeito de aprendizado, podemos apresentar o intervalo de confiança das diferenças entre as médias dos tratamentos para o ANOVA:

```{r, echo=FALSE, results='hide'}
if(!require(multcomp)){
    install.packages("multcomp")
    library(multcomp)
}
getwd()
```

```{r, echo=TRUE,fig.align='center',out.width = '80%'}
out_tukey <- glht(model=model,linfct = mcp(label_out="Tukey")) 
out_tukey_CI <- confint(out_tukey, level = 1-alpha)
print(out_tukey_CI)
plot(out_tukey_CI,xlab = "Desempenho médio"  ,main = "Intervalos de 95% de confiança")
```


### 8. Derivação de Conclusões    

Através da análise das premissas do teste, foi observado que as amostras obtidas são independentes mas violam as premissas de homoscedasticidade e normalidade. Portanto foi necessário utilizar um teste não paramétrico.
Por se tratar de amostras independentes, o teste não-paramétrico de Kruskal-Wallis foi utilizado. 
Através do teste, vimos que há evidências suficientes para se rejeitar a hipótese nula de que não há diferença de desempenho entre as configurações do algorítmo, com um nível de significância de $\alpha$ = `r alpha`.
Através da análise gráfica, 'possível determinar que a configuração do algoritmo com melhor desempenho é a configuração 2. Além disso, através da análise dos intervalos de confiança da diferença entre as médias de todos vs todos, podemos observar que a média da diferença das comparações envolvendo a configuração 2 são sempre muito maiores, e como ela apresenta o menor valor médio, é possível concluir que é a configuração com melhor desempenho.

Como a configuração 2 foi identificada como a melhor, podemos realizar uma comparação de todos versus um, através do teste de Dunnet, para identificar, em média, a magnitude da diferença.

```{r, echo=TRUE,fig.align='center',out.width = '80%'}
algoritmo$label_out <- relevel(algoritmo$label_out, ref = "out2")
model2 <- aov(out~label_out, data = algoritmo)
out_dunnet <- glht(model=model2,linfct = mcp(label_out="Dunnet")) 
out_dunnet_CI <- confint(out_dunnet, level = 1-alpha)
print(out_dunnet_CI)
plot(out_dunnet_CI,xlab = "Desempenho médio"  ,main = "Intervalos de 95% de confiança")
```

Podemos então observar, que:

1) A diferença entre a configuração 2 e a configuração 1 possui magnitude igual a 43.4702, com 95% de confiança no intervalo de 42.8442 a 44.0962

2) A diferença entre a configuração 2 e a configuração 3 possui magnitude igual a 41.9042, com 95% de confiança no intervalo de 41.2782 a 42.5302

3) A diferença entre a configuração 2 e a configuração 4 possui magnitude igual a 43.2757, com 95% de confiança no intervalo de 42.6497 a 43.9017

### 9. Discussão sobre Possíveis Limitações do Estudo e Sugestões de Melhoria   



### Referências   
[1] Storn R, Price K (1997). "Differential Evolution: A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces." *J. of Global Optimization*, **11**(4), 341-359.   
[2] Campelo F, Botelho M (2016). "Experimental Investigation of Recombination Operators for Differential Evolution." In *Proc. Genetic and Evolutionary Computation Conference - GECCO'2016*.   
[3] Portal Action. "Técnicas Não Paramétricas". Disponível em: <http://www.portalaction.com.br/tecnicas-nao-parametricas>. Acesso em 31/10/2017
[4] Portal Action. "Teste de Kruskal Wallis". Disponível em: <http://www.portalaction.com.br/976-4-teste-de-kruskal-wallis>. Acesso em 31/10/2017